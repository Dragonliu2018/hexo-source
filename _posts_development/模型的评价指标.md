---
title: 模型的评价指标
tags:
  - 深度学习
categories:
  - 人工智能
toc: true
mathjax: true
top: false
comments: true
copyright: true
date: 2022-04-26 14:59:37
---

> 评估在测试集上计算

# 1 基本

* **TP**（True positives）：代表软件样本被认为是此类型，实际标签正是此类型；
* **TN**（True negatives）代表软件样本被认为不是此类型，实际标签不是此类型；
* **FP**（False positives）：代表软件样本被认为是此类型，实际标签不是此类型；
* **FN**（False negatives）：代表软件样本被认为不是此类型，实际标签正是此类型。

# 2 准确率（Accuracy）

被预测正确的比例：**Accuracy = (TP+TN) / (TP+TN+FP+FN)**

样本不平均的情况，此时模型评估不能仅仅依靠准确率。因此需要再结合F1值（$F_1-score$）

# 3 精确率（Precision）

"正确被预测为正(TP)"占所有"实际被预测为正的(TP+FP)"的比例（混淆矩阵中除以所在的那一列之和），可信度

**Precision = TP / (TP+FP)**

# 4 召回率（Recall）

"正确被预测为正(TP)"占所有"应该被预测为正(TP+FN)"的比例（混淆矩阵中除以所在的那一行之和），查全率

**Recall = TP / (TP+FN)**

# 5 F1值（F1-score）

同时考虑了精确率和召回率，precison和recall的调和平均值(?)

**F1 = 2\*Recall\*Precision / (Recall + Precision)**

# 6 Macro F1

将n分类的评价拆成n个二分类的评价，计算每个二分类的F1 score，n个F1 score的平均值即为Macro F1。

# 7 Micro F1

将n分类的评价拆成n个二分类的评价，将n个二分类评价的TP、FP、TN、FN对应相加，计算评价准确率和召回率，由这2个准确率和召回率计算的F1 score即为Micro F1。
(TP + FP) / (TP + TN + FP + FN)，实际上就是accuracy，分母就是输入分类器的预测样本个数，分子就是预测正确的样本个数（无论类别）。

一般来讲，Macro F1、Micro F1高的分类效果好。Macro F1受样本数量少的类别影响大。
宏平均比微平均更合理，但也不是说微平均一无是处，具体使用哪种评测机制，还是要取决于数据集中样本分布。

# 8 加权平均F1(?)
